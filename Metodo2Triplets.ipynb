{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cypFspokZifl",
    "outputId": "99e4a90a-6150-4375-e590-0ddbf106d8e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZr0NUZVZPzp"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms,datasets\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.transform import rotate\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "destination_folder = \"/content/gdrive/My Drive/Vision_BCN/\"\n",
    "dir_blue_shirts = \"/content/gdrive/My Drive/Vision_BCN/blue/\"\n",
    "dir_no_blue_shirts = \"/content/gdrive/My Drive/Vision_BCN/no_blue/\"\n",
    "\n",
    "\n",
    "class CustomGuilleDataSet(Dataset):\n",
    "    def __init__(self, dir_1, dir_2, transform):\n",
    "        self.dir_1 = dir_1\n",
    "        self.dir_2 = dir_2\n",
    "        self.transform = transform\n",
    "        self.blue = os.listdir(dir_1)\n",
    "        self.no_blue = os.listdir(dir_2)      \n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.blue) + len(self.no_blue))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Load Blue and No_blue images one at a time to make up good trios.\n",
    "        if idx < len(self.blue): img_loc = os.path.join(self.dir_1, self.blue[idx]) if idx % 2 == 0 else os.path.join(self.dir_2, self.no_blue[idx])\n",
    "        else : img_loc = os.path.join(self.dir_2, self.no_blue[idx-len(self.blue)]) if idx % 2 == 1 else os.path.join(self.dir_1, self.blue[idx-len(self.blue)])\n",
    "        label = 0 if idx % 2 == 0 else 1  #label 0 == Blue. label 1 == No_blue\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "        tensor_image = self.transform(image)\n",
    "        return tensor_image,label\n",
    "\n",
    "\n",
    "\n",
    "trsfm = transforms.Compose(   \n",
    "    [transforms.Resize((500,500)),\n",
    "     transforms.RandomVerticalFlip(),\n",
    "     transforms.RandomRotation(45),\n",
    "     transforms.ToTensor(),\n",
    "     ])\n",
    "\n",
    "'''\n",
    "#Uncomment for evaluation\n",
    "trsfm = transforms.Compose(   \n",
    "    [transforms.Resize((500,500)),\n",
    "     transforms.ToTensor(),\n",
    "     ])\n",
    "'''\n",
    "\n",
    "#Initialize custom class\n",
    "data = CustomGuilleDataSet(dir_1 = dir_blue_shirts, dir_2 = dir_no_blue_shirts ,transform=trsfm )\n",
    "\n",
    "#Parameters\n",
    "batch_size = 20\n",
    "validation_split = 0.15\n",
    "dataset_size = len(data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "#Indices\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "#Samplers\n",
    "train_sampler = SequentialSampler(train_indices)\n",
    "valid_sampler = SequentialSampler(val_indices)\n",
    "\n",
    "#DataLoaders\n",
    "train_loader = DataLoader(data, batch_size=batch_size, sampler=train_sampler)\n",
    "validation_loader = DataLoader(data, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "#Check\n",
    "#for idx, (img,label) in enumerate(train_loader):\n",
    "#  print(img.shape)\n",
    "#  print(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97,
     "referenced_widgets": [
      "ead4bcfe861342acbee507083d3f80b6",
      "a1376606ff9c40b99019197b24d64c6d",
      "ba27c60970aa4e4290620004eaeab8cb",
      "048181a8b6a84f288735a0010a59c95e",
      "a07bcd68d1e64a6295a82dd86c9b13cb",
      "571f356ec6c84067b066fa1e02ab3817",
      "a7729775dd98439bb7857420b66c6b56",
      "f866ad0ac4c14c08acc85b83530538d5"
     ]
    },
    "id": "-60OicMwzFvO",
    "outputId": "c4d7533f-165b-45dc-fc23-00fae9d3a558"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-4df8aa71.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead4bcfe861342acbee507083d3f80b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=244418560.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "147472\n"
     ]
    }
   ],
   "source": [
    "#73000 parameters to train. With dropout = 0.5 so, roughly 35000. We have\n",
    "original_model = models.alexnet(pretrained=True)\n",
    "\n",
    "class CustomGuilleNet(nn.Module):\n",
    "  def __init__(self):\n",
    "      super(CustomGuilleNet, self).__init__()\n",
    "      self.features = nn.Sequential(\n",
    "          *list(original_model.features.children())\n",
    "      )\n",
    "      self.fc1 = nn.Linear(9216, 16)\n",
    "      self.dropout = nn.Dropout(0.5) #Since we don't have a lot of data and quite a lot of weights because of the two fc layers\n",
    "      self.pool = nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "\n",
    "  def forward(self, x):\n",
    "      x = self.features(x)\n",
    "      x = self.pool(x)\n",
    "      x = x.view(-1,256*6*6)\n",
    "      x = self.dropout(x)\n",
    "      x = self.fc1(x)            \n",
    "      return x\n",
    "\n",
    "  def get_embedding(self,x):\n",
    "    return self.forward(x)\n",
    "\n",
    "model = CustomGuilleNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "for i, param in enumerate(model.parameters()):\n",
    "  if i < 10:\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iN7zzlotfJWH"
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AllTripletSelector():\n",
    "    \"\"\"\n",
    "    Returns all possible triplets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AllTripletSelector, self).__init__()\n",
    "\n",
    "    def get_triplets(self, embeddings, labels):\n",
    "        labels = labels.cpu().data.numpy()\n",
    "        triplets = []\n",
    "        for label in set(labels):\n",
    "            label_mask = (labels == label)\n",
    "            label_indices = np.where(label_mask)[0]\n",
    "            if len(label_indices) < 2:\n",
    "                continue\n",
    "            negative_indices = np.where(np.logical_not(label_mask))[0]\n",
    "            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n",
    "\n",
    "            # Add all negatives for all positive pairs\n",
    "            temp_triplets = [[anchor_positive[0], anchor_positive[1], neg_ind] for anchor_positive in anchor_positives\n",
    "                             for neg_ind in negative_indices]\n",
    "            triplets += temp_triplets\n",
    "\n",
    "        return torch.LongTensor(np.array(triplets))\n",
    "    \n",
    "    \n",
    "\n",
    "class OnlineTripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Online Triplets loss\n",
    "    Takes a batch of embeddings and corresponding labels.\n",
    "    Triplets are generated using triplet_selector object that take embeddings and targets and return indices of\n",
    "    triplets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin, triplet_selector):\n",
    "        super(OnlineTripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.triplet_selector = triplet_selector\n",
    "\n",
    "    def forward(self, embeddings, target):\n",
    "\n",
    "        triplets = self.triplet_selector.get_triplets(embeddings, target)\n",
    "        #print('tri_shape: ',triplets.shape) #con batch 20 --> (900,3) novecientos trios\n",
    "        if embeddings.is_cuda:\n",
    "            triplets = triplets.cuda()\n",
    "\n",
    "        ap_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 1]]).pow(2).sum(1)  # .pow(.5)\n",
    "        an_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 2]]).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = F.relu(ap_distances - an_distances + self.margin)\n",
    "\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akg30XseeFWM"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(save_path, model, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_checkpoint(load_path, model):\n",
    "    \n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_metrics(load_path):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "048bvx5Admf2"
   },
   "outputs": [],
   "source": [
    "def fit(model,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        batch_size,\n",
    "        device,\n",
    "        train_loader = train_loader,\n",
    "        valid_loader = validation_loader,\n",
    "        num_epochs = 500,\n",
    "        eval_every = len(train_loader),\n",
    "        file_path = destination_folder,\n",
    "        best_valid_loss = float(\"Inf\")):\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "    \n",
    "    print('Lets start train!')\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "      for img,labels in train_loader:       \n",
    "        img = img.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(img)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % eval_every == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():       \n",
    "              # validation loop\n",
    "              for (img, labels) in valid_loader:\n",
    "                img = img.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(img)\n",
    "                loss = loss_fn(outputs,labels)\n",
    "\n",
    "                valid_running_loss += loss.item()\n",
    "  \n",
    "\n",
    "            # evaluation\n",
    "            average_train_loss = running_loss / eval_every\n",
    "            average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "            train_loss_list.append(average_train_loss)\n",
    "            valid_loss_list.append(average_valid_loss)\n",
    "            global_steps_list.append(global_step)\n",
    "\n",
    "            # resetting running values\n",
    "            running_loss = 0.0                \n",
    "            valid_running_loss = 0.0\n",
    "            model.train()\n",
    "\n",
    "            # print progress\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                          average_train_loss, average_valid_loss))\n",
    "            \n",
    "            # checkpoint\n",
    "            if best_valid_loss > average_valid_loss:\n",
    "                best_valid_loss = average_valid_loss\n",
    "                save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
    "                save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)             \n",
    "              \n",
    "\n",
    "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Finished Training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fY8505QfB4F"
   },
   "outputs": [],
   "source": [
    "margin = 1.\n",
    "loss_fn = OnlineTripletLoss(margin, AllTripletSelector())\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "fit(model,loss_fn,optimizer,batch_size,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtSV2n0puFNf"
   },
   "outputs": [],
   "source": [
    "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
    "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "plt.xlabel('Global Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGurrCAMvGEa"
   },
   "outputs": [],
   "source": [
    "best_model = CustomGuilleNet().to(device)\n",
    "\n",
    "load_checkpoint(destination_folder + '/model.pt', best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVxpn1woyWrf"
   },
   "outputs": [],
   "source": [
    "def extract_embeddings(dataloader, model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        embeddings = np.zeros((len(dataloader.dataset), 16))\n",
    "        labels = np.zeros(len(dataloader.dataset))\n",
    "        k = 0\n",
    "        for images, target in dataloader:\n",
    "            \n",
    "            images = images.to(device)\n",
    "            embeddings[k:k+len(images)] = model.get_embedding(images).data.cpu().numpy()\n",
    "            labels[k:k+len(images)] = target.numpy()\n",
    "            k += len(images)\n",
    "    return embeddings, labels\n",
    "\n",
    "trsfm = transforms.Compose(   \n",
    "    [transforms.Resize((500,500)),\n",
    "     transforms.ToTensor(),\n",
    "     ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNTVSk6ZzZgs"
   },
   "outputs": [],
   "source": [
    "embeddings, labels = extract_embeddings(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "foDWsrub2N2V",
    "outputId": "03470d37-eb45-4740-bf4d-e34076a725d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(694,)"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit_predict(embeddings)\n",
    "kmeans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SwAAMq3K4FIm",
    "outputId": "2c854449-a5a2-4e5f-c6a2-5a901c587e5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5648414985590778"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(kmeans)):\n",
    "  if kmeans[i] == labels[i]: count +=1\n",
    "print('Result: 'count/len(kmeans)) #0.5648 --> Slightly better than random --> Net didn't learn the patterns of the images it was trained on."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CV_BCN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "048181a8b6a84f288735a0010a59c95e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f866ad0ac4c14c08acc85b83530538d5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a7729775dd98439bb7857420b66c6b56",
      "value": " 233M/233M [01:47&lt;00:00, 2.28MB/s]"
     }
    },
    "571f356ec6c84067b066fa1e02ab3817": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a07bcd68d1e64a6295a82dd86c9b13cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a1376606ff9c40b99019197b24d64c6d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7729775dd98439bb7857420b66c6b56": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba27c60970aa4e4290620004eaeab8cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_571f356ec6c84067b066fa1e02ab3817",
      "max": 244418560,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a07bcd68d1e64a6295a82dd86c9b13cb",
      "value": 244418560
     }
    },
    "ead4bcfe861342acbee507083d3f80b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ba27c60970aa4e4290620004eaeab8cb",
       "IPY_MODEL_048181a8b6a84f288735a0010a59c95e"
      ],
      "layout": "IPY_MODEL_a1376606ff9c40b99019197b24d64c6d"
     }
    },
    "f866ad0ac4c14c08acc85b83530538d5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
